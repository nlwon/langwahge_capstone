{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486803da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\g_bab\\.conda\\envs\\week3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from langwahge.model import Model, loss_accuracy\n",
    "from mynn.optimizers.sgd import SGD\n",
    "import numpy as np\n",
    "#from langwahge.coco_data import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b19ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "import json \n",
    "import re, string\n",
    "import pickle\n",
    "import mygrad as mg\n",
    "class Coco:\n",
    "    punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    \n",
    "    def get_idf(self):\n",
    "        \"\"\" \n",
    "        Given the vocabulary, and the word-counts for each document, computes\n",
    "        the inverse document frequency (IDF) for each term in the vocabulary.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        nt: dict{string, float}\n",
    "            A dictionary storing the IDF for each term in vocab\n",
    "        \"\"\"\n",
    "        nt = {}\n",
    "        N = len(self.counters)\n",
    "        for t in self.vocab:\n",
    "            total = 0\n",
    "            for counter in self.counters:\n",
    "                if t in counter:\n",
    "                    total += 1\n",
    "\n",
    "            nt[t] = np.log10(N / total)\n",
    "\n",
    "        return nt\n",
    "    \n",
    "    def strip_punc(self, corpus):\n",
    "        \"\"\" \n",
    "        Removes all punctuation from a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            the corpus with all punctuation removed\n",
    "        \"\"\"\n",
    "        # substitute all punctuation marks with \"\"\n",
    "        \n",
    "        return self.punc_regex.sub('', str(corpus))\n",
    "    \n",
    "    def to_counter(self, doc):\n",
    "        \"\"\" \n",
    "        Produce word-count of document, removing all punctuation\n",
    "        and making all the characters lower-cased.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        doc : str\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        collections.Counter\n",
    "            lower-cased word -> count\n",
    "        \"\"\"\n",
    "        return Counter(self.strip_punc(doc).lower().split())\n",
    "\n",
    "    def to_vocab(self, counters, k=None):\n",
    "        \"\"\" \n",
    "        Convert a collection of counters to a sorted list of the top-k most common words \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        counters : Sequence[collections.Counter]\n",
    "            A list of counters; each one is a word tally for a document\n",
    "        \n",
    "        k : Optional[int]\n",
    "            If specified, only the top-k words are returned\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            A sorted list of the unique strings.\n",
    "        \"\"\"\n",
    "        vocab = Counter()\n",
    "        for counter in counters:\n",
    "            vocab.update(counter)\n",
    "            \n",
    "        return sorted(i for i,j in vocab.most_common(k))\n",
    "\n",
    "    def __init__(self): \n",
    "        \"\"\"\n",
    "        load COCO metadata (json file [\"images\"] [\"annotations\"])\n",
    "        load glove data (dictionary {word : word_embedding})\n",
    "        load in resnet data from resnet18_features.pkl (dictionary {img id : dvector})\n",
    "        \n",
    "        initialize the following attributes:\n",
    "        image-ID -> [cap-ID-1, cap-ID-2, ...]\n",
    "        caption-ID -> image-ID\n",
    "        caption-ID -> caption (e.g. 24 -> \"two dogs on the grass\")\n",
    "        \n",
    "        initialize vocab list and counters list as attributes\n",
    "       \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # load COCO metadata\n",
    "        with Path(r\"C:\\Users\\g_bab\\Downloads\\captions_train2014.json\").open() as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        # load GloVe-200 embeddings\n",
    "        self.glove = KeyedVectors.load_word2vec_format(r\"C:\\Users\\g_bab\\Downloads\\glove.6B.200d.txt.w2v\", binary=False)\n",
    "\n",
    "        # load image descriptor vectors\n",
    "        with Path(r\"C:\\Users\\g_bab\\Downloads\\resnet18_features.pkl\").open('rb') as f:\n",
    "            self.resnet18_features = pickle.load(f)\n",
    "        \n",
    "        self.imgid_to_capid = {}\n",
    "        self.capid_to_imgid = {}\n",
    "        self.capid_to_capstr = {}\n",
    "        self.counters = []\n",
    "\n",
    "        for caption in self.coco_data[\"annotations\"]:\n",
    "            # input caption_id to imgid_to_capid if the img_id key exists\n",
    "            if self.imgid_to_capid.__contains__(caption[\"image_id\"]):\n",
    "                self.imgid_to_capid[caption[\"image_id\"]].append(caption[\"id\"])\n",
    "            # else create new img_id object and create new caption_id list\n",
    "            else:\n",
    "                self.imgid_to_capid[caption[\"image_id\"]] = [caption[\"id\"]]\n",
    "\n",
    "            # input img_id to capid_to_imgid\n",
    "            self.capid_to_imgid[caption[\"id\"]] = caption[\"image_id\"]\n",
    "            # input caption to capid_to_capstr\n",
    "            self.capid_to_capstr[caption[\"id\"]] = caption[\"caption\"]\n",
    "    \n",
    "            self.counters.append(self.to_counter(caption))\n",
    "\n",
    "        self.vocab = self.to_vocab(self.counters)\n",
    "        \n",
    "    def random_pair(self):\n",
    "        \"\"\"\n",
    "        returns a random caption_string and respective image id\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        None\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        Tuple(int, string)\n",
    "            this contains the random caption_string and respective image id\n",
    "\n",
    "        \"\"\"    \n",
    "        # random respective caption string\n",
    "        captions = self.coco_data[\"annotations\"]\n",
    "        i = random.randint(len(captions)) \n",
    "        \n",
    "        caption_info = captions[i]\n",
    "        \n",
    "        return caption_info[\"image_id\"], caption_info[\"caption\"]\n",
    "        \n",
    "    def vectorize_image(self, image_id):\n",
    "        \"\"\"\n",
    "        takes in an image_id and returns the descriptor vector of the image\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        image_id: int\n",
    "            unique integer ID for the image in coco_data\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        image_dvector: np.array shape-(512,)\n",
    "            a descriptor vector of the image as provided by RESNET\n",
    "        \"\"\"\n",
    "        if image_id not in self.resnet18_features.keys():\n",
    "            return np.zeros((512,))\n",
    "        else:\n",
    "            return self.resnet18_features[image_id]\n",
    "\n",
    "    def embed_text(self, text_string):\n",
    "        \"\"\"\n",
    "        returns normal_text_embedding\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        text_string: String\n",
    "            a caption/query text \n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        String\n",
    "            normal text embedding\n",
    "        \"\"\"\n",
    "        # returns normal_text_embedding\n",
    "        # embed any caption / query text using GloVe-200 embeddings weighted by IDFs of words across captions (pass in either a user's query or existing caption)     \n",
    "        # lowercase, remove punc, tokenize\n",
    "        text_string = strip_punc(text_string).lower().split()\n",
    "\n",
    "        text_embedding = []\n",
    "        idf = self.get_idf()\n",
    "        for word in text_string:\n",
    "        # check if each word in given string is in glove[], if not then embedding vector 0\n",
    "        # else get glove vector (200,) for it\n",
    "            glove_vector = 0\n",
    "            if word in self.glove:\n",
    "                glove_vector = self.glove[word]\n",
    "            \n",
    "            idf_word = idf[word]\n",
    "            text_embedding.append(glove_vector * idf_word)\n",
    "\n",
    "        # add all together for the final phrase embed vector, then normalize\n",
    "        normal_text_embedding = mg.sqrt(mg.einsum(\"ij, ij -> i\", text_embedding, text_embedding)).reshape(-1, 1)\n",
    "\n",
    "        # return normal_text_embedding\n",
    "        return normal_text_embedding\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        returns coco_data, glove , resnet18_features, imgid_to_capid, capid_to_imgid, capid_to_capstr, counters\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        None\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        Tuple(dict, dict, dict)\n",
    "            contains coco data, glove data, and resnet data\n",
    "        \"\"\"\n",
    "        return (self.coco_data, self.glove, self.resnet18_features, self.imgid_to_capid, self.capid_to_imgid, self.capid_to_capstr, self.counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d6de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Coco()\n",
    "_, glove, resnet18_features, imgid_to_capid, capid_to_imgid, capid_to_capstr, _ = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4708035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\g_bab\\.conda\\envs\\week3\\lib\\site-packages\\noggin\\plotter.py:364: UserWarning: Live plotting is not supported when matplotlib uses the 'module://ipykernel.pylab.backend_inline'\n",
      "backend. Instead, use the 'nbAgg' backend.\n",
      "\n",
      "In a Jupyter notebook, this can be activated using the cell magic:\n",
      "   %matplotlib notebook.\n",
      "  warn(cleandoc(_inline_msg.format(self._backend)))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from noggin import create_plot\n",
    "# plotter, fig, ax = create_plot(metrics=[\"loss\"], max_fraction_spent_plotting=.75)\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"],  max_fraction_spent_plotting=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a306c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a0421fdf1493>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimg_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcapid_to_imgid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#values = np.array(list(imgid_to_capid.values()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mconf_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapid_to_imgid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconf_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mimg_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mconf_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapid_to_imgid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\week3\\lib\\random.py\u001b[0m in \u001b[0;36mchoice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;34m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "triplets = []\n",
    "# (caption_id, img_id, confuser_id)\n",
    "for key in list(capid_to_imgid.keys()):\n",
    "    caption_id = key\n",
    "    img_id = capid_to_imgid[key]\n",
    "    #values = np.array(list(imgid_to_capid.values()))\n",
    "    conf_id = random.choice(list(capid_to_imgid.values()))\n",
    "    if conf_id == img_id:\n",
    "        conf_id = random.choice(list(capid_to_imgid.values()))\n",
    "    triplets.append((caption_id, img_id, conf_id))\n",
    "    #print(triplets)\n",
    "    \n",
    "#split the data\n",
    "split_at = 0.8\n",
    "split = int(len(triplets) * split_at)\n",
    "train_triplets = triplets[:split] \n",
    "test_triplets = triplets[split:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db45525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.optimizers.sgd import SGD\n",
    "import numpy as np\n",
    "#from .coco import *\n",
    "import random\n",
    "\n",
    "model = Model(512, 200)\n",
    "optim = SGD(model.parameters, learning_rate = 1e-3, momentum =0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f49bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "captionidlist = [i[0] for i in train_triplets]\n",
    "list_phrases = [capid_to_capstr[i] for i in captionidlist]\n",
    "\n",
    "w_captions = np.array([data.embed_text(i) for i in list_phrases])\n",
    "\n",
    "captionidlisttest = [i[0] for i in test_triplets]\n",
    "list_phrasestest = [capid_to_capstr[i] for i in captionidlisttest]\n",
    "\n",
    "w_captionstest = np.array([data.embed_text(i) for i in list_phrasestest])\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    indexes = np.arange((len(train_triplets)))\n",
    "    np.random.shuffle(indexes)\n",
    "    for batch_count in range(0,len(train_triplets)//batch_size):\n",
    "        batch_indexes = indexes[batch_count*batch_size: batch_count*(batch_size+1)]\n",
    "        \n",
    "        img_ids = [i[1] for i in train_triplets[batch_indexes]]\n",
    "        \n",
    "        img_batch = [data.vectorize_image(imgid) for imgid in img_ids]\n",
    "        img_preds = model(np.array(img_batch))\n",
    "        \n",
    "        conf_ids = [j[2] for j in train_triplets[batch_indexes]]\n",
    "        \n",
    "        conf_batch = [data.vectorize_image(confid) for confid in conf_ids]\n",
    "        conf_preds = model(np.array(conf_batch))\n",
    "        #print(batch)\n",
    "        #w_captions = data.embed_text(capid_to_capstr[train_triplets[batch_indexes][0]])  #should correspond to the vectors \n",
    "        #confuser = model(resnet18_features[random.choice(list(resnet18_features.keys())[:82600])])  \n",
    "        #w_captions = data.embed_text(np.array([capid_to_capstr[i] for i in train_triplets[batch_indexes][0]]))\n",
    "        \n",
    "        # captionidlist = [i[0] for i in train_triplets[batch_indexes]]\n",
    "        # list_phrases = [capid_to_capstr[i] for i in captionidlist]\n",
    "\n",
    "        # w_captions = np.array([data.embed_text(i) for i in list_phrases])\n",
    "        w_captions = w_captions[batch_indexes]\n",
    "\n",
    "        sim_match = w_captions@img_preds\n",
    "        sim_confuse = w_captions@conf_preds\n",
    "        loss, acc = loss_accuracy(sim_match, sim_confuse, 0.25, train_triplets[batch_indexes])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        plotter.set_train_batch({\"loss\" : loss.item(), \"accuracy\" : acc}, batch_size=batch_size)\n",
    "\n",
    "filename = save_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c90b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "with mg.no_autodiff:\n",
    "    batch_size = 32\n",
    "    num_epochs = 10000\n",
    "    for epoch in range(num_epochs):\n",
    "        indexes = np.arange((len(test_triplets)))\n",
    "        np.random.shuffle(indexes)\n",
    "        for batch_count in range(0,len(test_triplets)//batch_size):\n",
    "            batch_indexes = indexes[batch_count*batch_size: batch_count*(batch_size+1)] \n",
    "            img_batch = data.vectorize_image(test_triplets[batch_indexes][1])\n",
    "            img_preds = model(img_batch)\n",
    "            conf_batch = data.vectorize_image(test_triplets[batch_indexes][2])\n",
    "            conf_preds = model(conf_batch)\n",
    "            #print(batch)\n",
    "            #w_captions = data.embed_text(capid_to_capstr[test_triplets[batch_indexes][0]])  #should correspond to the vectors \n",
    "            #confuser = model(resnet18_features[random.choice(list(resnet18_features.keys())[:82600])])  \n",
    "            #w_captions = data.embed_text(np.array([capid_to_capstr[i] for i in test_triplets[batch_indexes][0]]))\n",
    "        \n",
    "            list_phrases = [capid_to_capstr[i] for i in test_triplets[batch_indexes][0]]\n",
    "\n",
    "            w_captions = np.array([data.embed_text(i) for i in list_phrases])\n",
    "\n",
    "            sim_match = w_captions@img_preds\n",
    "            sim_confuse = w_captions@conf_preds\n",
    "            loss, acc = loss_accuracy(sim_match, sim_confuse, 0.25, len(test_triplets))\n",
    "        \n",
    "            loss.backward()\n",
    "        \n",
    "            optim.step()\n",
    "        \n",
    "            plotter.set_train_batch({\"loss\" : loss.item(), \"accuracy\" : acc}, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
